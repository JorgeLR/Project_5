{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Important Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rake_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from rake_nltk import Rake \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity, linear_kernel\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>description</th>\n",
       "      <th>photo_url</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Lisa Rowan</td>\n",
       "      <td>Its hurricane season, and weve got a weirdo st...</td>\n",
       "      <td>It’s hurricane season, and we’ve got a weirdo ...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>2019-07-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Never Try To Drive Through a Flood</td>\n",
       "      <td>https://lifehacker.com/never-try-to-drive-thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Yessenia Funes on Earther, shared by Virginia ...</td>\n",
       "      <td>Tropical Storm Barry still doesnt formally exi...</td>\n",
       "      <td>Tropical Storm Barry still doesn’t formally ex...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Orleans Faces a Major Flood Threat [Updating]</td>\n",
       "      <td>https://earther.gizmodo.com/new-orleans-faces-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              author  \\\n",
       "0                                         Lisa Rowan   \n",
       "1  Yessenia Funes on Earther, shared by Virginia ...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Its hurricane season, and weve got a weirdo st...   \n",
       "1  Tropical Storm Barry still doesnt formally exi...   \n",
       "\n",
       "                                         description  \\\n",
       "0  It’s hurricane season, and we’ve got a weirdo ...   \n",
       "1  Tropical Storm Barry still doesn’t formally ex...   \n",
       "\n",
       "                                           photo_url    pub_date source  \\\n",
       "0  https://i.kinja-img.com/gawker-media/image/upl...  2019-07-10    NaN   \n",
       "1  https://i.kinja-img.com/gawker-media/image/upl...  2019-07-11    NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0                 Never Try To Drive Through a Flood   \n",
       "1  New Orleans Faces a Major Flood Threat [Updating]   \n",
       "\n",
       "                                                 url  \n",
       "0  https://lifehacker.com/never-try-to-drive-thro...  \n",
       "1  https://earther.gizmodo.com/new-orleans-faces-...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('2.processed_data/mega_data.csv')\n",
    "fire_df = pd.read_csv('2.processed_data/fire_comb.csv')\n",
    "blizz_df = pd.read_csv('2.processed_data/blizzard_comb.csv')\n",
    "flood_df = pd.read_csv('2.processed_data/flood_comb.csv')\n",
    "hurr_df = pd.read_csv('2.processed_data/hurricane_comb.csv')\n",
    "earth_df = pd.read_csv('2.processed_data/earthquake_comb.csv')\n",
    "torn_df = pd.read_csv('2.processed_data/tornado_comb.csv')\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell combines the text from the relevant columns in preparation for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined_text'] = df['content'].map(str) + df['title'].map(str) + df['description'].map(str)\n",
    "fire_df['combined_text'] = fire_df['content'].map(str) + fire_df['title'].map(str) + fire_df['description'].map(str)\n",
    "blizz_df['combined_text'] = blizz_df['content'].map(str) + blizz_df['title'].map(str) + blizz_df['description'].map(str)\n",
    "flood_df['combined_text'] = flood_df['content'].map(str) + flood_df['title'].map(str) + flood_df['description'].map(str)\n",
    "hurr_df['combined_text'] = hurr_df['content'].map(str) + hurr_df['title'].map(str) + hurr_df['description'].map(str)\n",
    "earth_df['combined_text'] = earth_df['content'].map(str) + earth_df['title'].map(str) + earth_df['description'].map(str)\n",
    "torn_df['combined_text'] = torn_df['content'].map(str) + torn_df['title'].map(str) + torn_df['description'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def tokenize(x): \n",
    "    return tokenizer.tokenize(x)\n",
    "\n",
    "df['tokens'] = df['combined_text'].map(tokenize)\n",
    "fire_df['tokens'] = fire_df['combined_text'].map(tokenize)\n",
    "blizz_df['tokens'] = blizz_df['combined_text'].map(tokenize)\n",
    "flood_df['tokens'] = flood_df['combined_text'].map(tokenize)\n",
    "hurr_df['tokens'] = hurr_df['combined_text'].map(tokenize)\n",
    "earth_df['tokens'] = earth_df['combined_text'].map(tokenize)\n",
    "torn_df['tokens'] = torn_df['combined_text'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(x): \n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in x])\n",
    "def lemmatize(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return' '.join([lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lems'] = df['tokens'].map(lemmatize)\n",
    "df['stems'] = df['tokens'].map(stemmer)\n",
    "\n",
    "fire_df['lems'] = fire_df['tokens'].map(lemmatize)\n",
    "fire_df['stems'] = fire_df['tokens'].map(stemmer)\n",
    "\n",
    "blizz_df['lems'] = blizz_df['tokens'].map(lemmatize)\n",
    "blizz_df['stems'] = blizz_df['tokens'].map(stemmer)\n",
    "\n",
    "flood_df['lems'] = flood_df['tokens'].map(lemmatize)\n",
    "flood_df['stems'] = flood_df['tokens'].map(stemmer)\n",
    "\n",
    "hurr_df['lems'] = hurr_df['tokens'].map(lemmatize)\n",
    "hurr_df['stems'] = hurr_df['tokens'].map(stemmer)\n",
    "\n",
    "earth_df['lems'] = earth_df['tokens'].map(lemmatize)\n",
    "earth_df['stems'] = earth_df['tokens'].map(stemmer)\n",
    "\n",
    "torn_df['lems'] = torn_df['tokens'].map(lemmatize)\n",
    "torn_df['stems'] = torn_df['tokens'].map(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>description</th>\n",
       "      <th>photo_url</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lems</th>\n",
       "      <th>stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Lisa Rowan</td>\n",
       "      <td>Its hurricane season, and weve got a weirdo st...</td>\n",
       "      <td>It’s hurricane season, and we’ve got a weirdo ...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>2019-07-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Never Try To Drive Through a Flood</td>\n",
       "      <td>https://lifehacker.com/never-try-to-drive-thro...</td>\n",
       "      <td>Its hurricane season, and weve got a weirdo st...</td>\n",
       "      <td>[Its, hurricane, season, and, weve, got, a, we...</td>\n",
       "      <td>Its hurricane season and weve got a weirdo sto...</td>\n",
       "      <td>it hurrican season and weve got a weirdo storm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                            content  \\\n",
       "0  Lisa Rowan  Its hurricane season, and weve got a weirdo st...   \n",
       "\n",
       "                                         description  \\\n",
       "0  It’s hurricane season, and we’ve got a weirdo ...   \n",
       "\n",
       "                                           photo_url    pub_date source  \\\n",
       "0  https://i.kinja-img.com/gawker-media/image/upl...  2019-07-10    NaN   \n",
       "\n",
       "                                title  \\\n",
       "0  Never Try To Drive Through a Flood   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://lifehacker.com/never-try-to-drive-thro...   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Its hurricane season, and weve got a weirdo st...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Its, hurricane, season, and, weve, got, a, we...   \n",
       "\n",
       "                                                lems  \\\n",
       "0  Its hurricane season and weve got a weirdo sto...   \n",
       "\n",
       "                                               stems  \n",
       "0  it hurrican season and weve got a weirdo storm...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating keywords\n",
    "This code was adapted from a data science blog post written by Emma Grimaldi. There is an nltk package that allows you to extract keywords from a particular article. Keywords are the most relevant words that are associated with a particular piece of content. The code below extracts keywords from the combined_text column and puts them into a column called keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#received code from https://towardsdatascience.com/how-to-build-from-scratch-a-content-based-movie-recommender-with-natural-language-processing-25ad400eb243\n",
    "# turn this into a mappable function\n",
    "\n",
    "df['keywords'] = \"\"\n",
    "fire_df['keywords'] = \"\"\n",
    "blizz_df['keywords'] = \"\"\n",
    "flood_df['keywords'] = \"\"\n",
    "hurr_df['keywords'] = \"\"\n",
    "earth_df['keywords'] = \"\"\n",
    "torn_df['keywords'] = \"\"\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    comb_text = row['combined_text']\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(comb_text)\n",
    "    key_words_dict = r.get_word_degrees()\n",
    "    row['keywords'] = list(key_words_dict.keys())\n",
    "    \n",
    "for index,row in fire_df.iterrows():\n",
    "    comb_text = row['combined_text']\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(comb_text)\n",
    "    key_words_dict = r.get_word_degrees()\n",
    "    row['keywords'] = list(key_words_dict.keys())\n",
    "    \n",
    "for index,row in blizz_df.iterrows():\n",
    "    comb_text = row['combined_text']\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(comb_text)\n",
    "    key_words_dict = r.get_word_degrees()\n",
    "    row['keywords'] = list(key_words_dict.keys())\n",
    "    \n",
    "for index,row in flood_df.iterrows():\n",
    "    comb_text = row['combined_text']\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(comb_text)\n",
    "    key_words_dict = r.get_word_degrees()\n",
    "    row['keywords'] = list(key_words_dict.keys())\n",
    "    \n",
    "for index,row in hurr_df.iterrows():\n",
    "    comb_text = row['combined_text']\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(comb_text)\n",
    "    key_words_dict = r.get_word_degrees()\n",
    "    row['keywords'] = list(key_words_dict.keys())\n",
    "    \n",
    "for index,row in earth_df.iterrows():\n",
    "    comb_text = row['combined_text']\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(comb_text)\n",
    "    key_words_dict = r.get_word_degrees()\n",
    "    row['keywords'] = list(key_words_dict.keys())\n",
    "    \n",
    "for index,row in torn_df.iterrows():\n",
    "    comb_text = row['combined_text']\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(comb_text)\n",
    "    key_words_dict = r.get_word_degrees()\n",
    "    row['keywords'] = list(key_words_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a two-layer neural net that processes text. It's input is some text and the output is a list of feature vectors that are similar to that original text. The purpose of using Word2Vec is to boost our search engine's capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The user inputs keywords such as (california, wildfire)\n",
    "2. The function then cleans the text string because the word2vec model can only handle lowercase strings\n",
    "3. The word2vec algorithim finds the most similar words related to that search term and appends those words along with the original search term to a list.\n",
    "4. The function then sees how many keywords in our dataframe match the keywords in the keywords in the search term list. \n",
    "5. The function outputs the title of the most related articles given a threshold for a specific number of search terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('lexvec.enwiki+newscrawl.300d.W.pos.vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def art_test(df):\n",
    "    search = input(\"Type disaster then location(ex: fire, california):\")\n",
    "    stripped = [x.strip() for x in search.split(' ')]\n",
    "    result = [x.lower() for x in stripped]\n",
    "    print(result)\n",
    "    word2vec_list = []\n",
    "    for x in model.most_similar(result):\n",
    "        term,sim = x \n",
    "        word2vec_list.append(term)\n",
    "    word2vec_list.extend(result)\n",
    "    print(word2vec_list)\n",
    "    article_list = []\n",
    "    threshold = 7\n",
    "    print(threshold)\n",
    "    while len(article_list) < 5 and threshold > 3:\n",
    "        for article in df.loc[df['keywords'].map(lambda x: sum(1 for w in word2vec_list if w in x)) == threshold, 'title']:\n",
    "            article_list.append(article)\n",
    "        threshold -= 1\n",
    "    return article_list if len(article_list) > 0 else print('No Results!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wildfires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type disaster then location(ex: fire, california):wildfire\n",
      "['wildfire']\n",
      "['wildfires', 'blazes', 'blaze', 'bushfire', 'fires', 'bushfires', 'fire', 'conflagration', 'grassfire', 'flames', 'wildfire']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "wildfire_list = art_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wildfire_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_df[\"final_list\"] = fire_df[\"title\"].map(lambda x: 1 if x in wildfire_list else 0)\n",
    "fire_df = fire_df.loc[fire_df[\"final_list\"] == 1]\n",
    "fire_df.reset_index(inplace= True)\n",
    "fire_df.drop(axis=1, columns=\"index\")\n",
    "fire_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tornado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type disaster then location(ex: fire, california):tornado\n",
      "['tornado']\n",
      "['tornadoes', 'twister', 'twisters', 'thunderstorm', 'storm', 'typhoon', 'tornados', 'cyclone', 'storms', 'thunderstorms', 'tornado']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "torn_list = art_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 15)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torn_df[\"final_list\"] = torn_df[\"title\"].map(lambda x: 1 if x in torn_list else 0)\n",
    "torn_df = torn_df.loc[torn_df[\"final_list\"] == 1]\n",
    "torn_df.reset_index(inplace= True)\n",
    "torn_df.drop(axis=1, columns=\"index\")\n",
    "torn_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type disaster then location(ex: fire, california):flood\n",
      "['flood']\n",
      "['flooding', 'floods', 'deluge', 'floodwaters', 'flooded', 'inundation', 'levees', 'floodwater', 'tsunami', 'levee', 'flood']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "flood_list = art_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flood_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 15)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flood_df[\"final_list\"] = flood_df[\"title\"].map(lambda x: 1 if x in flood_list else 0)\n",
    "flood_df = flood_df.loc[flood_df[\"final_list\"] == 1]\n",
    "flood_df.reset_index(inplace= True)\n",
    "flood_df.drop(axis=1, columns=\"index\")\n",
    "flood_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type disaster then location(ex: fire, california):earthquake\n",
      "['earthquake']\n",
      "['quake', 'temblor', 'tsunami', 'aftershock', 'aftershocks', 'earthquakes', 'quakes', 'tsunamis', 'temblors', 'tremor', 'earthquake']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "earth_list = art_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(earth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 15)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earth_df[\"final_list\"] = earth_df[\"title\"].map(lambda x: 1 if x in earth_list else 0)\n",
    "earth_df = earth_df.loc[earth_df[\"final_list\"] == 1]\n",
    "earth_df.reset_index(inplace= True)\n",
    "earth_df.drop(axis=1, columns=\"index\")\n",
    "earth_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blizzard - No news articles for blizzards in this time of the year (Only the blizzard entertainment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type disaster then location(ex: fire, california):blizzard\n",
      "['blizzard']\n",
      "['snowstorm', 'blizzards', 'snowstorms', 'whiteout', 'thunderstorm', 'snowfall', 'storms', 'rainstorm', 'torrential', 'storm', 'blizzard']\n",
      "7\n",
      "No Results!\n"
     ]
    }
   ],
   "source": [
    "blizz_list = art_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    len(blizz_list)\n",
    "except:\n",
    "    print(\"No articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    blizz_df[\"final_list\"] = blizz_df[\"title\"].map(lambda x: 1 if x in blizz_list else 0)\n",
    "    blizz_df = blizz_df.loc[blizz_df[\"final_list\"] == 1]\n",
    "    blizz_df.reset_index(inplace= True)\n",
    "    blizz_df.drop(axis=1, columns=\"index\")\n",
    "    blizz_df.shape\n",
    "except:\n",
    "    print(\"No articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurricanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type disaster then location(ex: fire, california):hurricane\n",
      "['hurricane']\n",
      "['typhoon', 'cyclone', 'storm', 'superstorm', 'tropical', 'landfall', 'hurricanes', 'nhc', 'storms', 'katrina', 'hurricane']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "hurr_list = art_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hurr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 15)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hurr_df[\"final_list\"] = hurr_df[\"title\"].map(lambda x: 1 if x in hurr_list else 0)\n",
    "hurr_df = hurr_df.loc[hurr_df[\"final_list\"] == 1]\n",
    "hurr_df.reset_index(inplace= True)\n",
    "hurr_df.drop(axis=1, columns=\"index\")\n",
    "hurr_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_author(prueba):\n",
    "    for art in range(len(prueba)):\n",
    "        if pd.isnull(prueba['url'][art]):\n",
    "            prueba['author'][art] = \"No author\"\n",
    "        else:\n",
    "            if pd.isnull(prueba['author'][art]):\n",
    "                if (str(prueba[\"url\"][art]).split(\"/\",3)[:3])[2].split(\".\")[0] == \"www\":\n",
    "                    prueba['author'][art] = (str(prueba[\"url\"][art]).split(\"/\",3)[:3])[2].split(\".\")[1]\n",
    "    #                 print(2)\n",
    "                else:\n",
    "                    prueba['author'][art] = (str(prueba[\"url\"][art]).split(\"/\",3)[:3])[2].split(\".\")[0]\n",
    "    #                 print(1)\n",
    "            else:\n",
    "    #             print(\"yes\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgeramos/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/jorgeramos/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "fill_author(df)\n",
    "fill_author(fire_df)\n",
    "fill_author(blizz_df)\n",
    "fill_author(earth_df)\n",
    "fill_author(flood_df)\n",
    "fill_author(hurr_df)\n",
    "fill_author(torn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.to_csv('3.final_data/fire_comb.csv', index = False)\n",
    "blizz_df.to_csv('3.final_data/blizzard_comb.csv', index = False)\n",
    "earth_df.to_csv('3.final_data/earthquake_comb.csv', index = False)\n",
    "flood_df.to_csv('3.final_data/flood_comb.csv', index = False)\n",
    "hurr_df.to_csv('3.final_data/hurricane_comb.csv', index = False)\n",
    "torn_df.to_csv('3.final_data/tornado_comb.csv', index = False)\n",
    "df.to_csv('3.final_data/mega_data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
